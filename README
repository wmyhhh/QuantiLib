本仓库是一个集成的CLI工具, 用于实现简便的大模型量化
目前支持的量化方法有: Bitsandbytes, GPTQ, AWQ

Installation
```bash
pip install -r requirements.txt
```     

Usage
Example:
```bash
python3 -m QuantiLib.cli --model_name "Qwen/Qwen3-1.7B" --method bnb --save_tokenizer False --bnb_4bit_use_double_quant True --save_dir ./bnb_4bit
python3 -m QuantiLib.cli --model_name "Qwen/Qwen3-1.7B" --method gptq --batch_size 1 --gptq_group_size 128
```
Detailed arguments:
```bash
========================= Model Arguments ========================
    --model_name MODEL_NAME
    --model_path MODEL_PATH
    --method {bnb,gptq,awq,aqlm}
    --device_map {auto,cuda,cpu}
    --save_dir SAVE_DIR
    --save_tokenizer {True,False}

======================== BNB Arguments ===========================
    --quant_type {4bit,8bit}        
    --bnb_4bit_compute_type {float16,bfloat16,float32}
    --bnb_4bit_quant_type {nf4,fp4}
    --bnb_4bit_use_double_quant {True,False}          

======================== GPTQ Arguments ==========================
    --quant_type {2bit,3bit,4bit}  # "2bit", "3bit", or "4bit"
    --device_map {auto,cuda,cpu}
    --save_tokenizer {True,False}
    --batch_size BATCH_SIZE        # 校准时的 batch size
    --calib_dataset CALIB_DATASET  # 校准数据集
    --gptq_group_size GPTQ_GROUP_SIZE  # GPTQ 的 group size
    
======================== AWQ Arguments ==========================
    --quant_type {2bit,3bit,4bit}  # "2bit", "3bit", or "4bit"
    --group_size GROUP_SIZE        # AWQ 的 group size

